{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab7680b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "class NeuralModel():\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x)\n",
    "        denom = np.sum(exps, axis=1)\n",
    "        denom.resize(exps.shape[0], 1)\n",
    "        return exps / denom\n",
    "\n",
    "    def __init__(self, sizes, epochs=20, l_rate=0.01):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.l_rate = l_rate\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        input_layer = int(self.sizes[0])\n",
    "        hidden_1 = int(self.sizes[1])\n",
    "        hidden_2 = int(self.sizes[2])\n",
    "        output_layer = int(self.sizes[3])\n",
    "\n",
    "        # Random initialization of weights between -1 and 1\n",
    "        self.w1 = np.random.uniform(low=-1, high=1, size=(input_layer, hidden_1))\n",
    "        self.w2 = np.random.uniform(low=-1, high=1, size=(hidden_1, hidden_2))\n",
    "        self.w3 = np.random.uniform(low=-1, high=1, size=(hidden_2, output_layer))\n",
    "\n",
    "        # Zero initialization of weights\n",
    "        #self.w1 = np.zeros((input_layer, hidden_1))\n",
    "        #self.w2 = np.zeros((hidden_1, hidden_2))\n",
    "        #self.w3 = np.zeros((hidden_2, output_layer))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Input layer to hidden layer1\n",
    "        inputs = inputs.numpy()\n",
    "        self.linear_1 = inputs.dot(self.w1)\n",
    "        self.out1 = self.sigmoid(self.linear_1)\n",
    "\n",
    "        # hidden layer 1 to 2\n",
    "        self.linear_2 = self.out1.dot(self.w2)\n",
    "        self.out2 = self.sigmoid(self.linear_2)\n",
    "\n",
    "        # Hidden layer to softmax layer\n",
    "        self.linear3 = self.out2.dot(self.w3)\n",
    "        self.out3 = self.softmax(self.linear3)\n",
    "\n",
    "        return self.out3\n",
    "\n",
    "    def backward(self, x_train, y_train, output):\n",
    "        # Convert tensors to numpy arrays\n",
    "        x_train = x_train.numpy()\n",
    "        y_train = y_train.numpy()\n",
    "\n",
    "        batch_size = y_train.shape[0]\n",
    "\n",
    "        # Derivative of loss\n",
    "        d_loss = output - y_train\n",
    "        # Calculating delta for W3\n",
    "        change_w3 = (1. / batch_size) * np.matmul(self.out2.T, d_loss)\n",
    "\n",
    "        # Backpropagating to the 2nd layer from the third layer\n",
    "        d_out_2 = np.matmul(d_loss, self.w3.T)\n",
    "        d_linear_2 = d_out_2 * self.sigmoid(self.linear_2) * (1 - self.sigmoid(self.linear_2))\n",
    "        # Calculating delta for W2\n",
    "        change_w2 = (1. / batch_size) * np.matmul(self.out1.T, d_linear_2)\n",
    "\n",
    "        # Backpropagating to the 1nd layer from the second layer\n",
    "        d_out_1 = np.matmul(d_loss, self.w3.T) * self.sigmoid(self.linear_2) * (1 - self.sigmoid(self.linear_2))\n",
    "        d_out_1 = np.matmul(d_out_1, self.w2.T)\n",
    "        d_linear_1 = d_out_1 * self.sigmoid(self.linear_1) * (1 - self.sigmoid(self.linear_1))\n",
    "        # Calculating delta for W2\n",
    "        change_w1 = (1. / batch_size) * np.matmul(x_train.T, d_linear_1)\n",
    "\n",
    "        return change_w1, change_w2, change_w3\n",
    "\n",
    "    def update_weights(self, w1_update, w2_update, w3_update):\n",
    "        self.w1 -= self.l_rate * w1_update\n",
    "        self.w2 -= self.l_rate * w2_update\n",
    "        self.w3 -= self.l_rate * w3_update\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        batch_size = y.shape[0]\n",
    "        y = y.numpy()\n",
    "        # Computing the cross entropy loss for the model and its given predictions\n",
    "        loss = np.sum(np.multiply(y, np.log(y_hat)))\n",
    "        loss = -(1. / batch_size) * loss\n",
    "        return loss\n",
    "\n",
    "    def compute_metrics(self, val_loader):\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(val_loader):\n",
    "            x, y = data\n",
    "            # Converting to expected one-hot format\n",
    "            y_onehot = torch.zeros(y.shape[0], 10)\n",
    "            y_onehot[range(y_onehot.shape[0]), y] = 1\n",
    "            # Flattening input image into 1-D\n",
    "            flattened_input = x.view(-1, 28 * 28)\n",
    "            output = self.forward(flattened_input)\n",
    "            predicted = np.argmax(output, axis=1)\n",
    "            # Calculating correctly predicted labels\n",
    "            correct += np.sum((predicted == y.numpy()))\n",
    "            total += y.shape[0]\n",
    "            # Computing the cross entropy loss\n",
    "            loss = self.compute_loss(y_onehot, output)\n",
    "            losses.append(loss)\n",
    "        # Performing mean over all minibatches\n",
    "        return (correct / total), np.mean(np.array(losses))\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        start_time = time.time()\n",
    "        global losses\n",
    "        global accuracies\n",
    "        for iteration in range(self.epochs):\n",
    "            for i, data in enumerate(train_loader):\n",
    "                x, y = data\n",
    "                # Since the model is producing a softmax probability over 10 classes, the label needs to be converted to a one-hot encoded vector\n",
    "                y_onehot = torch.zeros(y.shape[0], 10)\n",
    "                y_onehot[range(y_onehot.shape[0]), y] = 1\n",
    "                # Converting 28x28 image into a flattened input\n",
    "                flattened_input = x.view(-1, 28 * 28)\n",
    "                # Forward pass the input through the model\n",
    "                output = self.forward(flattened_input)\n",
    "                # Compute gradients for the linear layer weights using SGD\n",
    "                w1_update, w2_update, w3_update = self.backward(flattened_input, y_onehot, output)\n",
    "                # Perform weight update for the minibatch\n",
    "                self.update_weights(w1_update, w2_update, w3_update)\n",
    "            # Compute the mean loss over the test set after the completion of epoch\n",
    "            accuracy, loss = self.compute_metrics(val_loader)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%, Loss: {3:.2f}'.format(\n",
    "                iteration + 1, time.time() - start_time, accuracy * 100, loss\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0902744",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),])\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "input_size = 784\n",
    "hidden_layer1_size = 300\n",
    "hidden_layer2_size = 300\n",
    "output_size = 10\n",
    "\n",
    "\n",
    "\n",
    "trainset = datasets.MNIST('./dataset/MNIST/', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('./dataset/MNIST/', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = NeuralModel(sizes=[784, 300,200, 10], epochs=25)\n",
    "# Training the model over the MNIST dataset\n",
    "model.train(train_loader=trainloader, val_loader=valloader)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes this will cause kernal crush. Use google colab helps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
